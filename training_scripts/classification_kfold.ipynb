{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(input_dim,512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512,256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256,128),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(128,64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64,1),\n",
    "          nn.Sigmoid()\n",
    "        )\n",
    "               \n",
    "    # 3. Define a forward method containing the forward pass computation\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformAndScale(X,y,train_ids, test_ids):\n",
    "  scaler = StandardScaler()\n",
    "  xtrain, xtest = X[train_ids], X[test_ids]\n",
    "  ytrain, ytest = y[train_ids], y[test_ids]\n",
    "  X_train = scaler.fit_transform(xtrain)\n",
    "  X_test = scaler.transform(xtest)\n",
    "  train_tensor = torch.tensor(X_train)\n",
    "  test_tensor = torch.tensor(X_test)\n",
    "  y_tensor =  torch.from_numpy(ytrain.values.ravel()).float()\n",
    "  ytest_tensor =  torch.from_numpy(ytest.values.ravel()).float()\n",
    "  y_tensor = y_tensor.unsqueeze(1)\n",
    "  ytest_tensor = ytest_tensor.unsqueeze(1)\n",
    "  return train_tensor,test_tensor,y_tensor,ytest_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictEncoder(train_tensor,test_tensor):\n",
    "  train_tensor = Variable(train_tensor).cuda()\n",
    "  test_tensor = Variable(test_tensor).cuda()\n",
    "  with torch.no_grad():  \n",
    "    encoded_train_balanced = autoencoder.encoder(train_tensor.float())\n",
    "    encoded_test_balanced = autoencoder.encoder(test_tensor.float())\n",
    "  return encoded_train_balanced,encoded_test_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,matthews_corrcoef, cohen_kappa_score,roc_curve,RocCurveDisplay\n",
    "import numpy as np\n",
    "from time import process_time\n",
    "\n",
    "#task = Task.init(project_name='MusIkA', task_name='5-KFold Classification try-2')\n",
    "#task = Task.init(project_name='MusIkA', task_name='5-KFold Classification try-whitout-encoding')\n",
    "\n",
    "bs = 256\n",
    "k_folds = 5\n",
    "num_epochs = 200\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  \n",
    "# For fold results\n",
    "results = {}\n",
    "  \n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "  \n",
    "  \n",
    "# Define the K-fold Cross Validator\n",
    "kfold = StratifiedKFold(n_splits=k_folds, shuffle=True)\n",
    "    \n",
    "# Start print\n",
    "print('--------------------------------')\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "t1 = process_time()\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(X,y)):\n",
    "    \n",
    "    # Print\n",
    "  print(f'FOLD {fold}')\n",
    "  print('--------------------------------')\n",
    "  \n",
    "  train_tensor,test_tensor,y_tensor,ytest_tensor = transformAndScale(X,y,train_ids, test_ids)\n",
    "\n",
    "  #encodedtrain, encodedtest = predictEncoder(train_tensor,test_tensor)\n",
    "\n",
    "  train_ds = torch.utils.data.TensorDataset(train_tensor, y_tensor)\n",
    "  train_loader = torch.utils.data.DataLoader(train_ds, batch_size=bs)\n",
    "  test_ds = torch.utils.data.TensorDataset(test_tensor, ytest_tensor)\n",
    "  test_loader = torch.utils.data.DataLoader(test_ds, batch_size=128)\n",
    "  net = ClassificationModel(192)\n",
    "  net = net.to(device)\n",
    "  net.apply(reset_weights)\n",
    "\n",
    "  loss_function = nn.BCELoss()  # binary cross entropy\n",
    "  learning_rate = 0.0001\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "  net.train()\n",
    "  \n",
    "  for epoch in range(0, num_epochs):\n",
    "\n",
    "      # Print epoch\n",
    "      running_loss = 0.0\n",
    "      print(f'Starting epoch {epoch+1}')\n",
    "      # Set current loss value\n",
    "      for xb, yb in train_loader:\n",
    "          xb = xb.to(torch.float32)\n",
    "          yb = yb.to(device)\n",
    "          xb = xb.to(device)\n",
    "          optimizer.zero_grad() \n",
    "          y_pred = net(xb)            # Forward Propagation\n",
    "          loss = loss_function(y_pred, yb)  # Loss Computation\n",
    "          loss.backward()               # Back Propagation\n",
    "          optimizer.step()\n",
    "          running_loss += loss.item() * bs\n",
    "      epoch_loss = running_loss / len(train_tensor)              # Updating the parameters \n",
    "      print(\"Loss in iteration :\"+str(epoch)+\" is: \"+str(loss.item()))\n",
    "      #Logger.current_logger().report_scalar(f'loss graph in fold {fold+1}', \"train loss\", iteration=epoch+1,value=epoch_loss)     \n",
    "      print('Last iteration loss value: '+str(loss.item()))\n",
    "\n",
    "  net.eval()\n",
    "  y_pred_list = []\n",
    "  y_score_list = []\n",
    "  with torch.no_grad():\n",
    "    for xb_test,yb_test  in test_loader:\n",
    "        xb_test = xb_test.to(torch.float32)\n",
    "        xb_test = xb_test.to(device)\n",
    "        yb_test = yb_test.to(device)\n",
    "        y_test_pred = net(xb_test)\n",
    "        y_score_list.append(y_test_pred.numpy(force=True))\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_tag = y_pred_tag.cpu()\n",
    "        y_pred_list.append(y_pred_tag.detach().numpy())     \n",
    "    #Takes arrays and makes them list of list for each batch        \n",
    "  y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "  y_score_list = [a.squeeze().tolist() for a in y_score_list]\n",
    "  ytest_pred = list(itertools.chain.from_iterable(y_pred_list))\n",
    "  y_score_pred = list(itertools.chain.from_iterable(y_score_list))\n",
    "  ytest_pred = torch.FloatTensor(ytest_pred) \n",
    "  y_score_pred = torch.FloatTensor(y_score_pred) \n",
    "  ytest_pred = ytest_pred.unsqueeze(1)\n",
    "  y_score_pred = y_score_pred.unsqueeze(1)\n",
    "  conf_matrix = confusion_matrix(ytest_tensor ,ytest_pred)\n",
    "  tn, fp, fn, tp = confusion_matrix(ytest_tensor ,ytest_pred).ravel()\n",
    "  print(f'true negatives: {tn} false positives: {fp} false negatives {fn} true positives {tp}')\n",
    "  print(f'confusion matrix fold {fold}')\n",
    "  print(\"-----------\")\n",
    "  print(conf_matrix)\n",
    "  # Logger.current_logger().report_matrix(\n",
    "  #   f'confusion matrix of fold {fold+1}',\n",
    "  #   \"ignored\",\n",
    "  #   iteration=fold,\n",
    "  #   matrix=conf_matrix,\n",
    "  #   xaxis=\"True class\",\n",
    "  #   yaxis=\"Predicted class\",\n",
    "  #   xlabels= ['Negative','Positive'],\n",
    "  #   ylabels = ['Negative','Positive'],\n",
    "  #   yaxis_reversed = False\n",
    "  # )\n",
    "  precision = precision_score(ytest_tensor,ytest_pred)\n",
    "  recall = recall_score(ytest_tensor,ytest_pred)\n",
    "  f1_score_value = f1_score(ytest_tensor,ytest_pred)\n",
    "  mcc = matthews_corrcoef(ytest_tensor,ytest_pred)\n",
    "  kappa = cohen_kappa_score(ytest_tensor,ytest_pred)\n",
    "  fpr, tpr, thresholds = roc_curve(ytest_tensor, y_score_pred)\n",
    "  roc_curve_save = RocCurveDisplay.from_predictions(ytest_tensor, y_score_pred,name=f'Classifier in fold {fold+1}')\n",
    "  plt.show()\n",
    "  print(type(roc_curve_save))\n",
    "  #task.get_logger().report_plotly(title=f'Roc curve fold {fold+1}', series=\"\", iteration=0, figure=roc_curve_save)\n",
    "  print(\"Precision of the MLP :\\t\"+str(precision))\n",
    "  print(\"Recall of the MLP    :\\t\"+str(recall))\n",
    "  print(\"F1 Score of the Model :\\t\"+str(f1_score_value))\n",
    "  acc = accuracy_fn(ytest_tensor,ytest_pred)\n",
    "  print(f'Accuracy of the model: {acc}')\n",
    "  print(f'Matthews correlation coefficient: {mcc}')\n",
    "  print(f'cohen kappa score: {kappa}')\n",
    "  # Logger.current_logger().report_single_value(f'Accuracy in fold {fold+1}',acc)\n",
    "  # Logger.current_logger().report_single_value(f'Precision in fold {fold+1}',precision)\n",
    "  # Logger.current_logger().report_single_value(f'Recall in fold {fold+1}',recall)\n",
    "  # Logger.current_logger().report_single_value(f'F1_score in fold {fold+1}',f1_score_value)\n",
    "  # Logger.current_logger().report_single_value(f'Cohen kappa score in fold {fold+1}',kappa)\n",
    "  # Logger.current_logger().report_single_value(f'Matthews correlation coefficient in fold {fold+1}',mcc)\n",
    "  results[fold] = acc\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "  print(f'Fold {key}: {value} %')\n",
    "  sum += value\n",
    "avg = sum/len(results.items())\n",
    "print(f'Average: {avg} %') \n",
    "t2 = process_time() \n",
    "print(\"Elapsed time:\", t2-t1) \n",
    "# Logger.current_logger().report_single_value('Average accuracy in 5 folds',avg)\n",
    "# task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
